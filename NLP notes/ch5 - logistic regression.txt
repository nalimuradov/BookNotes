discriminative vs generative classifiers:
 - generative models look to understand what 'cats' and 'dogs' look like
	- could ask it to 'generate' a dog or cat
 - discriminative models learn to distinguish classes ONLY
	- finds a feature(s) that separates data nicely
		- eg. collars on dogs
		- will know that dogs have collars but won't know what a dog is

components of ML classification system:
 - input feature matrix
 - classification function that computes the estimated class
 - objective function that minimizes loss
 - algorithm for optimizing objective function (eg. SGD)

sigmoid:
 - z = wx + b is how we get our output prediction
	- z is the resultant weighted sum
	- b is the bias matrix
	- w is the weight matrix
	- x is the input feature matrix

 - sigmoid forces result to be between 0 and 1
	- sig = 1 / (1 + e^-z)

 - benefits:
	- almost linear around 0 but has a sharp slope toward ends
		- allows it to squash outliers
	- it is also differntiable at all points

logistic regression example:
 - basic neural net without layers
 - eg. weights = [2.5, -5.0, -1.2]
	bias = 0.1
	feautres = [3, 2, 1] which are counts of pos words, neg words, and counts of "no"
 - the weights show us that negative words are twice as important than positive words 
 - continued...
	- [2.5, -5.0, -1.2] * [3, 2, 1] + 0.1
		= -3.6
	- put thru sigmoid:
		- sig(-3.6) = 0.0266
 - therefore, the probability it's positive is 0.0266 and prob it's negative is 1 - 0.0266

logistic regression vs naive bayes:
 - LR better on larger documents and datasets
 - LR is more robust with correlated features

learning in logistic regression:
 - we want to find w, b that make our guess as close as possible to the correct answer
 - the loss function is the distance between our system output and our gold output
 - the optimization algorithm will update the weights to minimize loss function
	- a standard algorithm is stochastic gradient descent

loss function:
 - L_CE(w,b) eq'n 5.11
 - will be greater when it's confused
 - eg. on a confindent one loss was 0.37, but was 1.17 on an unconfident sentence
 - THIS IS WHAT WE MINIMIZE, NOW WE NEED TO FIND HOW TO MINIMIZE IT

gradient descent:
 - THIS IS HOW WE WILL MINIMIZE IT
 - it finds where the function's slope is rising most steeply and goes the opposite way
 - our example is convex (one minimum) so there are no local minima to get stuck at

gradient:
 - a multi-variable generalization of the slop
 - for a function with one variable, gradient = slope

learning rate:
 - the amount we move by in gradient descent 
 - higher learning rate means our w is moved more on each step
 - too high and we may never converge, too low and it will take too long to converge
 - a good alternative is start big and gradually make it smaller as we converge

SGD steps:
 - for each training tuple:
	1) what is our estimated output?
	2) how far off is our estimated output from the true output?
	3) how should we move theta to maximize loss?
	4) go the other way instead

batch training:
 - if batch is 1, we are just doing stochastic 
	- each example, we move thew weights a little
 - this can be choppy, so sometimes we compute over batches of training data

regularization:
 - overfitting:
	- when feature weights fit too perfectly to training data
	- won't be able to generalize well to other data
 - to prevent overfitting, a new regularization term is added
	- this helps penalize large weights
	- penalizes complexity that make it hard to generalize well
 - there are two common ways to compute regularization term:
	- L2 regularization
	- L1 regularization
	
to update theta based on the gradient:
 - theta_t+1 = theta_t - learning_rate * gradient of loss function

gradient of loss function:
 - we can use the cross entropy loss function

regularization:
 - weights that matches training data but uses many high value weights to do so
	- will be penalized more than settings with less accuracy but smaller weighst
 - two ways to compute R(theta) regularization term, L1 and L2

L1 regularization:
 - linear function of the weight values
 - harder to optimize since derivative of |theta| is non-continuous at zero
 - prefers sparse solutions with some larger weights
	- therefore L1 leads to fewer features

L2 regulariztion:
 - a quadratic function of the weight values 
 - easier to optimize because derivative of theta^2 is 2*theta

multinomial logistic regression:
 - AKA softmax regression
 - the target variable y ranges over more than two classes
	- find probability of y being in each potential class

softmax function:
 - takes a vector z of k arbitrary values and maps them to a probability distribution
 - each value between 0 and 1 and summing to 1
 - exponential function
	- if one input is larger than others, it will push its prob to 1 and squash others

features in multinomial regression:
 - for example, say we have 3 classes: positive, negative, neutral reviews
 - exclamation mark could be a feature that has positive weight for + and - classes
	- but has negative weight for 0 (neutral) classes

multinomial loss function:
 - has slightly differnet loss function than binary as binary uses sigmoid and multi uses softmax

benefits of logistic regression:
 - able to study the importance of individual features when classifying
