0.0 - Notation ---------------------------------------------------------------------------------------------------

some notation that I might forget:
 - B = (b1, b2, b3): ordered tuple
 - B = [b1, b2, b3]: matrix
 - B = {b1, b2, b3}: set
 - a, b, c, ...: scalaras
 - x, y, z, ...: vectors
 - <x, y>: inner product of x and y
 - rk(A): rank of matrix A
 - Im(x): image of linear mapping x
 - ker(x): kernel of linear mapping x
 - I: identity matrix
 - lambda: eigenvalue
 - E_lambda: eigenspace corresponding to eigenvalue lambda
 - upside down triangle: gradient
 
three core concepts of ML:
 - data:
	- goal of ML is to extract valuable patterns from data
 - a model:
	- related to process that generates data, 
	- in regression, model would describe a func that maps inputs to real-valued outputs
	- model considered to be learning if performance improves on a given task after the data is factored in
 - learning:
	- a way to automatically find patterns and structure in data by optimizing the parameters of the model

1.1 - Finding Words for Intuitions ---------------------------------------------------------------------------------------------------

predictors:
 - a system that makes predictions based on input data

training:
 - when a system adapts some internal parameters of the predictor so that it performs well on futuer unseen input data
 - "training" a system

vectors:
 - convenient to think of data as vectors (suitable for reading into a computer program)
 - three ways to think about vectors:
	- array of numbers (CS view)
	- arrow with direction and magnitude (physics view)
	- object that obeys addition and scalign (math view)

model: 
 - typically used to describe a process for generating data, similar to the dataset at hand
 - good models can be thought of as simplified versions of the real data-generating process
 - captures aspects that are relevant for modeling the data and extracting patterns from it
 - a good model can be used to predict what would happen in the real world without performing real world experiments
 
learning:
 - given a dataset and suitable model, training the model means to use the data available to optimize some parameters of the model
	- with respect to a utility function that evaluates how well the model predicts the training data
 - most training methods can be thought of like climbin a hill to reach its peak
	- the peak corresponds to a maximum of some desired performance measure
	- in practice, we want the model to perform well on unseen data

chapter TLDR:
 - represent data as vectors
 - choose an appropriate model, either using probabilistic or optimization view
 - learn from available data using numerical optimization methods

1.2 - Two Ways to Read This Book ---------------------------------------------------------------------------------------------------
	
bottom up:
 - building up the concepts from foundational to more advanced
 - preferred approached in mathematics
 - positive is it's easy as concepts build on previous ones
 - negative is it's not very interesting, lack of motivation

top down:
 - drilling down from practical needs
 - reader knows at all points why they need to work on a particular concept
 - downside is knowledge may be built on shaky foundations

four pillars of machine learning:
 - regression
 - dimensionality reduction 
 - density estimation
 - classification

part 1 topics (the base):
 - linear algebra (study of vectors and matrices)
 - analytic geometry (construction of similarity and distances of multiple vectors)
 - matrix decomposition (operations on matrices)
 - probability theory (quantification of uncertainty, useful for noise filtration)
 - vector calculus (gradients, the direction in which to search for a solution)
 - optimization (find maxima/minima of functions, need to find params to maximize some performance measure to train models)

part 2 topics (the pillars):
 - linear regression (find functions that map inputs to corresponding observed function values, interpreted as the labels of the respective inputs)
 - dimensionality reduction (to find a compact representaion of high dimensional data, easier to analyze than the original data)
 - density estimation (to find a probability distribution that describes a given dataset, eg. Gaussian mixture models)
 - classification (in the context of support vector machines, have inputs x and labels y)

1.3 - Exercises and Feedback ---------------------------------------------------------------------------------------------------

exercise problems:
 - part 1 exercises will be done pen and paper
 - part 2 exercises will be done on jupyter notebook

