sentiment analysis:
 - the extaction of sentiment
 - the positive/negative orientation that writer expresses
 - eg. review of a movie
 - simplest sentiment analysis:
	- binary classification: eg. greatly/awesome vs trash/awful
	- spam detection is another simple example (key spam words)

classification:
 - extremely useful to classify data into buckets
 - generative classifiers (naive bayes):
	- return class most likely to have generated the observation
 - discriminative classifiers (logistic regression):
 	- learn what features from input are most useful to distinguish between possible classes

naive bayes:
 - we can calculate the class of a document by:
	- c = max(P(d|c) * P(c))
		- removed denom because it's the same for each class
	- to find the class, we need the prior probability of the class P(c) and the likelihood of the doc P(d|c)

 - we can also represent document d as a set of features f1 to fn
	- c = max(P(f1, f2, .. | c) * P(c))

 - to simplify, we ignore the word positions and only care about count
	- we also assume that probabilities of features are independent of each other
	- this is why it's called NAIVE bayes

 - naive bayes calculations are also done in log space (like language modeling) to avoid underflow
 - it is a linear classifier:
	- classifiers that use a linear combination of the inputs to make a classification decision

training the naive bayes classifier:
 - we can assume P(c) will be N_c/N_doc
 	- N_c is the number of docs in the training data with class 'c'
	- N_doc is the total number of documents in the training data
	
 - to find P(f_i|c)
 	- assume a feature is just the existence of the word in the bag -> P(w_i|c)

 - eg. P("fantastic"|positive) = count("fantastic", positive) / sum of count(w, positive)
 	- if "fantastic" doesn't appear in any positive reviews, will be zero!
	- as such, we implement Laplace (add-1) smoothing
		- add 1 to numerator and add |V| to denominator
		
 - unknown words:
 	- if words appear in test set that didn't appear in train set, ignore them
	- do not include any probability for them at all

 - stop words:
  	- some systems may choose to ignore stop words (eg. the, a)
	- can be done by removing 10-100 most common words in bag
	- optional and not necessary
	
 - example:
 	- Training Data:
		- "just plain boring" -> negative
		- "entirely predictable and lacks energy" -> negative
		- "no surprises and very few laughs" -> negative
		- "very powerful" -> positive
		- "the most fun film of the summer" -> positive
	- Testing Data:
		- "predictable with no fun" -> ?
		- first remove all words that do not appear in training set (ie. "with")
		- find class probabilities: P(negative) = 3/5, P(positive) = 2/5
		- find probability of each word with respect to each class
			- multiply and pick which is more likely
			- in this case, it predicts the sentence as NEGATIVE
