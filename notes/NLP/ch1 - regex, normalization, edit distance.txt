regular expression:
 - used to specify strings we want to extract from a document

text normalization: 
 - converting text to a more convenient form (eg. tokenizing)

tokenization: 
 - seperating text into 'tokens'
 - segmenting running text into words
 - typically words separated by white space, but not always
 - eg. 'New York' could be one token, and 'I'm' could be split into two tokens 'I am'
 - for tweets, a good token to track could be :) for smile emote, or a hashtag

lemmatization:
 - task of determining that two words have the same root
 - words 'sang', 'sung', 'sing' are all forms of the same verg 'sing'
	- 'sing' is the common lemma
 - useful for complex languages like arabic

corpus :
 - a collection of written texts or speech

regular expressions:
 - /woodchuck/ will find any matches containint substring 'woodchuck'
	- case sensitive so will not match with 'Woodchuck'
 - /a/ will find anything with an 'a' in it
 - /[wW]oodchuck/ lowercase and uppercase
 - /[abc]/ anything with any of these letters in it
 - /[1234567890]/ any number
	- writing out all numbers or all letters can get long
 	- as such, can write /[0-9]/
 - for any letter, /[A-Z]/

 - use a caret ^ to specify NOT
 - /[^A-Z]/ means not an uppercase letter
 - /[^sS]/ means neither 's' or 'S'

 - ? represents 'or nothing'
 - eg. /woodchucks?/ will return either 'woodchuck' or 'woodchucks'
 - eg. /colou?r/ will return either 'color' or 'colour'

 - asterisk used to represent zero or more instances of something
 - /a*/ will match with anything with zero or more a's, so pretty much everything
 - for one ore more a's, /aa*/

 - period is an expression for 'wildcard'
 - /beg.n/ will match 'begin', 'begun', 'began'

 - period and asterisk can be combined to perform very useful tasks
 	- eg. /aardvark.*aardvark/ will find any line where 'aardvark' appears twice

 - \b matches a word boundary, \B matches a non-boundary
	- thus, /\bthe\b/ will match 'the' but not 'other'

 - disjunction operator is like OR
	- /cat|dog/ will search for either 'cat' or 'dog'

 - parentheses to perform operations on certain parts of text
 	- eg. /gupp(y|ies)/ will search for either 'guppy' or 'guppies'
 	- very useful with asterisk * to find all matches of a string
	- /the*/ will match 'theeee' and /(the)*/ will match 'thethe'

 - curly braces indicate exact count of something
	- /a{3}/ will search for exactly 3 occurences of 'a'
	- /a{3,5}/ will search for 3 to 5 occurences of 'a'
 
 - use backslash to search for meta words like '*' (eg. \*)

 - some common keywords like \b:
	- \d will find any digit
	- \D will find any non-digit
	- \w will find any alphanumeric value
	- \W will find any non-alphanumeric value
	- \s will find any white space
	- \S will find any non-white space

 - regex substitution:
	- s/colour/color/ will replace 'colour' with 'color'

 - example: the Xer they were, the Xer they will be
	- we want to check where X is the same word
	- s/([0-9]+)/<\1>/
	- 'the bigger they were, the bigger they will be' will work
	- 'the bigger they were, the faster they will be' won't work
	- /the (.*)er they (.*), the \1er we \2/
		- will match 'the faster they ran, the faster we ran'
		- won't match 'the faster they ran, the faster we ate'
	- these are capture groups
	
capture groups:
 - brackets () have two uses:
 	- one to specify order of operations
	- other to create capture group and be reused later down the line
		- eg. the (.*)er they were, the \1er they will be
	- sometimes we want to use brackets but not create a capture group
		- place ?: at start of brackets()
		- eg. (?:some|a few) (people|cats) like some \1
			- the \1 will refer to either 'people' or 'cats', not 'some' nor 'a few'

words:
 - utterance:
	- spoken correlate of a sentence
	- 'I do uh main- mainly business data processing'

 - fillers:
	- filles or filled pauses are words like 'uh' and 'uhm'

 - fragments:
	- broken off words like 'main-' in the above utterance

 - word types:
 	- 'They picnicked by the pool, then lay back on the grass and looked at the stars.'
	- number of DISTINCT words in a corpus
	- above utterance has 14 types

 - word tokens:
	- total number of words in a corpus
	- above utterance has 16 tokens

 - relationship between types V and tokens N:
	- |V| = kN^B
	- B depends on corpus size and genre, typically 0.67 to 0.75
	- k is a positive constant
	- eg. shakespear corpus has 884 thousand tokens (N) and 31 thousand types (|V|)

corpora:
 - words have to come from somewhere, in some language, at some time, at some place, for a specific function
 - NLP algorithms are most useful when they apply across multiple languages
 - words can come from certain genres, religious texts, or be spoken in a combination of multiple languages

text normalization:
 - tokenizing words
 - normalizing word formats
 - segmenting sentences

 - crude text normalization and tokenization in UNIX:
	- can take works of shakespeare.txt and put one word on each line
	- can then make a count of each word [1945 a, 72 AARON, etc]
 	- can then make case-insensitive

	- to tokenize text, need to segment it into words
	- we may want to keep punctuation

punctuation:
 - crude text normalization is fine but there are plenty of cases we need to account for
 	- punctuation in general
	- punctuation within words (Ph.D)
 - we also don't want to segment "$45.55" into two, as it is one entity, price
 - URL's, email addresses, etc

clitic:
 - contractions that are marked by apostrophes
 - eg. we're, what're

named entity detection:
 - the task of detecting names, dates, and organizations
 - very intimately tied with tokenization

morpheme:
 - the smallest meaning-bearing unit of a language
 - eg. "unlikeliest" consists of the morphemes: "un-", "likely", and "-est"
 - subword tokens is helpful when dealing with unknown words, which is common in machine learning

byte-pair encoding:
 - AKA BPE
 - used to merge frequent pairs of characters
 - find most common pairs of characters in dictionary and then iteratively add to vocabulary
 - continue by finding next most common pair and add that
 - there are many tokenizers that work in various ways (***)

word normalization:
 - the task of putting words/tokens in a standard format
 - done after the tokenization process
 - case folding is a form of normalization where everything is mapped to lower case

morphology:
 - the study of the way words are built up from smaller morphemes

types of morphemes:
 - stems: the central morpheme of the word, supplying the main meaning
 - affixes: the additional meanings of various kinds
 - eg. "cats" contains stem "cat" and affix "-s"

stemmers:
 - used to lemmatize in a crude way
 - chop off affixes in words
 - example is Porter Stemmer

sentence segmentation:
 - most useful cues for segmenting a text into sentences are punctuation
 	- periods, question marks, exclamation points
 - periods can be ambigious, as a period can be end of sentence OR abbreviation marker OR both

minimum edit distance:
 - the task of measuring how similar two strings are
 - the minimum number of operations (insertion, deletion, substitution) needed to transform one string into another
 - eg. "intention" and "execution" have an edit distance of 5
 - if there is typo 'graffe', likely they meant to type 'giraffe' mora than 'grail'

coreference:
 - deciding wheter two strings refer to same entity
 - eg. Stanford President Hennessy, Stanford University President John Hennessy

computing edit distance:
 - D[i,j] is the edit distance between sting X of length n, and string Y of length m
	- i, j are the first i/j characters of strings X/Y
 - thus, the edit distance between X and Y is D[n,m]

