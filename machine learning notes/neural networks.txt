PART 1 -----------------------------------------------------------------------------------

neurons:
 - a function, takes in all outputs of previous layer as input, and spits out value as output
 - eg. 28x28 pixel image of a number, each pixel is a neuron (784 total)
 	- each holds a number between 0 and 1 
	- the number is called 'activation'
 	- each neuron makes for the first layer of the network
 - the ten output neurons make the last layer of the network
	- the activation in these neurons show probability of each number
 - middle layers are 'hidden layers'
	- the process is done here (black box for now)
 - activations in one layer determine activations in next layer
	- an input will cause specific pattern in next layer
	- which will cause specific pattern in layer after that
 - brightest neuron at the end is the most likely output

 - we register numbers in our brain by their shapes (eg. 9 is a loop and a stick)
	- can break down layers into this
	- second last layer will be common shapes of the number
	- if the loop and stick have high activations, the 9 in the next layer will too

 - the layer before that, we can see what edges make up the shapes
	- for a loop, we can see the edges that make up a loop

weights:
 - the connections between neurons
 - they each have a value, could be positive or negative
 - we can take each activation and multiply by its weight than sum it all up
	- w1a1+w2a2+w3a3...
	- this can be used to find edges, it will sum up the weights of the pixels in that area
	- to check for edge, can have negative weights associated with surrounding pixels
	- then, the sum will be largest if surrounding pixels aren't activated but selected are

sigmoid:
 - we usually want activate to be between 0 and 1
 - can use sigmoid function to squeeze number into that range
 - sigma(x) = 1/(1+e^-x)
 - very negative inputs are close to zero, very positive are close to one, smooth in between
 - use as coefficient

bias:
 - we may want to activate only when it is above a certain threshold
 - can include bias for inactivity
 - sigma(w1a1 + w2a2 + .... + wnan - 10)
	- 10 is the bias
 - do this for each neuron, each neuron has a weight and a bias
	- this network has 13002 weights and biases
	- 784*16 + 16*6 + 16*10 weights, 16+16+10 biases

notation:
 - easier way to represent the 
 - matrix multiplication

 - first matrix shows connections between one layer and particual neuron in next layer
 - second matrix is weighted sum of activations in the first layer
 - the third matrix contains the biases

          [w0,0 w0,1 w0,n][a0]   [b0]   [?]
  sigmoid [w1,0 w1,1 w1,n][a1] + [b1] = [?]
          [wk,0 wk,1 wk,n][an]   [b2]   [?]

 - multiply the whole thing by the sigmoid (apply sigmoid to each value in output vector)
 - the output is the value of the neurons in the next layer
 - a_(1) = sigma(Wa_(0) + b)

reLu
 - sigmoid isn't used that much anymore
 - reLU means rectified linear unit
 - reLU(a) = max(0,a)
	- either it is the value or it's zero if negative

PART 2 -----------------------------------------------------------------------------------

cost difference
 - a way to determine how incorrect or correct the guess is
 - add up squares of differences between output and desired output
	- this is the cost of a single training example

MNIST database
 - contains many images of hand written numbers
 - can use to test!

minimum
 - solution is to find the minimum in the curve
 - we look at slope and go towards where it's headed
	- slowly increment until slope reaches 0, where it is flat and at lowest point

gradient
 - gives you the direction of the steepest ascent
 - the negative of the gradient is what we need
 - the matrix shows us useful things:
	- the sign (+/i) shows if we should move it up or down
	- the magnitude shows which changes matter more

PART 3 -----------------------------------------------------------------------------------

learning
 - to teach, give it examples and calculate the cost differences
 - doing this with many will give you accurate system

back propogation
 - algorithm for calculating the gradient
 - how each training data wishes to nudge weights and biases	
 - how it works:
	- neurons in last row know what they want (eg. if 2, increase 2, decrease all others)
	- each neuron in second last row has thoughts of what each neuron in last row should do (increase/decrease)
 	- sum up for the second last row then recursevily apply to third last row
	- 'propogating backwards'

time 
 - training can take ages if done like above
 - alternative would be to split training data into batches
	- use in groups
	- the data will be more careless but quicker
	- old data would take slow careful steps but be slow when going to neg gradient
	- this is called stochastic gradient descent

data
 - most important thing of machine learning in general is having enough data
 - need lots of labeled images for machine to learn