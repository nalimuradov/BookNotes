Machine Learning Essentials

there are typically 3 types of ML algorithms:
 - supervised learning
 - unsupervised learning
 - reinforcement learning

supervised learning:
 - there is a target outcome variable (dependent variable)
	- to be predicted from given set of predictors (independent variables)
 - training continues until desired accuracy reached
 - examples: [regression, decision tree, random forest]

unsupervised learning:
 - there is not target outcome variable to predict
 - typically used for clustering population in different groups
 - examples: [k-means, apriori]

reinforcement learning:
 - machine is exposed to environment where it can train 'trial and error'
 - it learns from past experiences
 - examples: [markov decision process]

common ML algorithms:
 - linear regression
 - logistic regression
 - decision tree
 - SVM
 - naive bayes
 - kNN
 - k-means
 - random forest
 - dimensionality reduciton alg
 - gradient boosting algs:
	- GBM
	- XGBoost
	- LightGBM
	- CatBoost

--- ALGORITHMS ---

linear regression:
 - used to estimate real values based on continuous variables
 - the best fit line is the 'regression line'
	- represented by Y = aX + b
 - for example, say we ask a child to organize kids in classroom by weight, but he can't ask their weights
	- he will look at the height, build, etc which is correlated with weight 
 - Y: dependent variable
 - a: slop
 - X: independent variable
 - b: intercept
	- we get a and b by minimizing the sum of squared diff of distance between data points and regression line
 - two types of linear regression:
	- simple linear regression (one independent variable)
	- multiple linear regression (more than one independent variable)

logistic regression:
 - a classification (NOT a regression algorithm)
 - estimate discrete values (0/1, true/false, etc...)
 - for example, say a friend gives you puzzle to solve with only two outcome scenarios (solve it or don't)

decision tree:
 - split population into 2+ homogeneous sets (based on most significant attributes)
 - jezzball example

svm (support vector machine):
 - plot in n-dimensional space 
	- n is number of features you have
 - eg. plot height and hair length
 - find a line that goes thru middle of lines created by closest points
 - that line is our classifier:
	- depending on which side test data lands on, classify as new data

-- --

perceptron:
 - type of artificial neuron
 - take several binary inputs and output one binary output
 - eg. x1, x2, x3 can all be inputs, y can be output

 - each input can have weight w, to represent how important it is for output
 - output of this neuron will be 0 or 1, depending if it reaches a threshold value

 - cheese festival example:
	- factors: is weather good, does your girl wanna go, is it near a bus stop
	- each factor is an input: x1, x2, x3	
	- eg. if weather is good x1 = 1, if weather is bad x1 = 0
	- each one also has a weight based on how important it is to you	
	- if you really need the bus, the weight of x3 will be thicc
 	- say w1 = 2, w2 = 2, w3 = 3, threshold = 5
	- we can use this to see if threshold to go is met

 - we can have layers in a network, each one can make a decision at a more abstract/complex level
 - each perceptron has a single output:
	- the multiple output arrows in diagrams just demonstrate that the one output is used in multiple places
 
 - cleaner form: output = {0 if wx+b <= 0, 1 if wx+b > 0}
	- move threshold to other side and call it the bias
	- w and x are represented as vectors

 - bias:
	- a measure of how easy it is to get the perceptrion to fire
	- a perceptron with really big bias will be really easy to output 1
	- a perceptron with really negative bias will be really hard to output 1

 - logical functions:
	- we can use perceptrons to create functions like AND, OR, and NAND
	- eg. two inputs x1, x2, weights w1 = -2, w2 = -2, bias is 3
		- this is a NAND gate as inputs 00, 01, 10 all return 1, but 11 returns 0
	- we can use NAND gates to build anything and everything, like add two bits

 - inputs:
	- in diagrams, will be circled similarly to perceptrons
	- but they aren't actually perceptrons
	- a perceptron with no input will simply output 1 if b > 0, 0 if b <= 0
	
 - applications:
	- we can devise learning algorithms which can tune biases and weights of the network
	- tuning happens in response to external stimuli (no direct intervention from developer)