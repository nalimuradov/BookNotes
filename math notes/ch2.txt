2 - LINEAR ALGEBRA ---------------------------------------------------------------------------------------------------

algebra:
 - construct a set of objects (symbols) and a set of rules to manipulate these objects

linear algebra:
 - study of vectors and certain rules to manipulate these vectors

vectors:
 - can be added together
 - can be multiplied by scalars to produce another object of the same kind
 - eg. geometric vectors:
	- directed segments (which can be drawn)
 - eg. polynomials:
	- also classify as vectors
	- two polynomials can be added, and they can be multiplied by a scalar
	- abstract concepts
 - eg. tuples of n real numbers are vectors:
	- a = [1 2 3] element of R^3

2.1 - Systems of Linear Equations ---------------------------------------------------------------------------------------------------

example:
 - company produces products N_1, N_2, ..., N_n
 - resources R_1, R_2, ..., R_n are required to produce them
 - to produce a unit of product N_j, a_ij units of resource R_i are needed
 - i = 1, ...., m
 - j = 1, ...., n

 - goal is to find an optimal production plan:
	- how many units x_j of product N_j should be produced if a total of b_i units of resource R_i are available 
	- no resoures are left over
	
 - if we produce x_1, ..., x_n units:
	- optimal production plan: a_m1*x_1 + ... + a_mn*x_n = b_m
	- general form of a system of linear equations

solutions of linear systems:
 - either exatly one, infinitely many, or one solution
 - Ax = b
 - A is the matrix of coefficients, x is the matrix of variables, b is the matrix of constants

2.2 - Matrices ---------------------------------------------------------------------------------------------------

A = [a11 a12 a13] 
    [a21 a22 a23]
    [a31 a32 a33]

matrices: 
 - (m,n) where m is row and n is col
 - (1,n) matrices are called rows and (m,1) matrices are called columns
 - R^mxn is the set of all real-valued matrices

matrix addition:
 - simply add corresponding values in same position
 - two matrices must be the same size

matrix multiplication:
 - multiply row by col 
 - A = 2x3, B = 3x2, the middle numbers have to be the same for multiplication to work
 	- the outer numbers will be the size of the resultant matrix (in this case, 2x2)
	- multiplying B*A = BA will actually result in a 3x3 matrix
 - AB != BA, matrix multiplication is not commutative

identity matrix:
 - matrix with ones on the diagonal and zeroes everywhere else

properties of matrices:
 - associativity: (AB)C = A(BC)
 - distributivity: (A+B)C = AC + BC, A(C+D) = AC + AD
 - mult with identity matric: I_m*A = A*I_n = A
	- note that I_m != I_n if m != n

inverse of a matrix:
 - if we have AB = I = BA, then B is called the inverse of A (denoted by A^-1)
 - inverse is the matrix we use to multiply another matrix to get the identity matrix
 - not every matrix possesses an inverse
 - if inverse exists, matrix is called regular/invertible/nonsingular
 - if inverse does not exist, matrix is called singular/noninvertible

inverse example: [1 2 1]	[-7 -7  6]
		 [4 4 5]	[ 2  1 -1]
		 [6 7 7]	[ 4  5 -4]
	- these two matrices are inverse to each other since AB = I = BA

transpose of a matrix:
 - if we have matrix A and write cols as rows and vice versa, we get matrix B, the transpose of A
 - B = A^T
 - eg. A = [1 3]   A^T = [1 4] 
	   [4 2]	 [3 2]

properties of inverses and transposes:
 - AA^-1 = I = A^-1*A
 - (AB)^-1 = B^-1 * A^-1
 - (A+B)^1 != A^-1 + B^-1
 - (A^T)^T = A
 - (A+B)^T = A^T + B^T
 - (AB)^T = B^T * A^T
 - a matrix A is symmetric if A = A^T
	- only (n,n) matrices can be symmetric
	- (n,n) matrices are also called square matrices

multiplication by a scalar:
 - the scalar will scale each element of the matrix A
 	- associativity holds
	- distributivity holds

representation of systems of linear equations:
 - 2x1 + 3x2 + 5x3 = 1
 - 4x1 - 2x3 - 7x3 = 8
 - 9x1 + 5x2 - 3x2 = 2

 - [2  3  5] [x1]   [1]
   [4 -2 -7] [x2] = [8]
   [9  5 -3] [x3]   [2]

2.3 - Solving Systems of Linear Equations -----------------------------------------------------------------------------------------

example system:
 		[x1]
 - [1  0  8 -4] [x2]   [42]
   [0  1  2 12] [x3] = [8]
		[x4]
 - this system has two equations and four unkowns, therefore we would expect infinitely many solutions
 - we could do 42 times to first col and 8 times the second so that it adds up	
	- possible solution is [42, 8, 0, 0]^T
	- this is the particular/special solution
	- however, it's not the only solution

 - general solution for above system:
	- find col 3 as a combination of cols 1 and 2: [8] = 8 [1] + 2 [0]
						       [2]     [0]     [1]
	- now we can find values for x that make Ax = 0
	- that vector is [8 2 -1 0]^T
	- repeat for col 4 and we get: [-4 12 0 -1]^T
	
 	- put everything together for the general solution:
		      [42]  		[8]   		 [-4]
		- x =  [8] + lambda_1 * [2] + lambda_2 * [12]
		       [0]	       [-1]		  [0]
		       [0]		[0]		 [-1]

general solution steps:
 - find a particular solution to Ax = b
 - find all solutions to Ax = 0
 - combine the solutions from above steps into the general solution

elementary transformations:
 - exchance two equations
 - multiplication of an equation with a constant
 - addition of two equations

example gaussian transformation:
 - [-2   4  -2  -1   4 | -3]
   [ 4  -8   3  -3   1 |  2]
   [ 1  -2   1  -1   1 |  0]
   [ 1  -2   0  -3   4 |  a]
 
 - we now represent matrix in this [A|b] form as it is much cleaner
 - swap R1 with R3
 - R2 = R2 - 4R1
 - R3 = R3 + 2R1
 - R4 = R4 - R1
 - we end up with the below matrix (after some addition/subtraction):

 - [ 1  -2   1  -1   1 |  0]
   [ 0   0   1  -1   3 | -2]
   [ 0   0   0   1  -2 | 10]
   [ 0   0   0   0   0 |a+1]
 - this solved matrix is in Row-Echelon Form (REF)
 - we can now solve this system

 - variables corresponding to the pivots in row-echelon form are called basic variables, others are free variables
 - x1, x3, x4 are the basic variables and x2, x5 are the free variables

reduced row echelon form:
 - similar to row echelon form
 - every pivot is 1
 - the pivot is the only non zero entry in its column (nothing above or below it)
 - example:
	[1  3  0  0  3]
	[0  0  1  0  9]
	[0  0  0  1 -4]
 - allows us to determine the general solution in a simple way

 - so for first non-pivot column, we look at values to right of 2nd column (the first non pivot column)
 	- it is three times the first col so our vector becomes:
	-  [3]
	  [-1]
	   [0]
	   [0]
	   [0]

 - second non-pivot column (5th col) is 3 times col1, 9 times col3, -4 times col4
	- also put -1 for its own col
	-  [3]
	   [0]
	   [9]
	  [-4]
	  [-1]

 - to summarize, all solutions of Ax = 0 are given by:
			  [3]	            [3]
			 [-1]		    [0]
	- x = lambda_1 *  [0] + lambda_2 *  [9]
			  [0]   	   [-4]
			  [0]		   [-1]

	- lambda_1 and lambda_2 can be any real number

minus-1 trick:
 - add row with a -1 in the pivot spot for all columns that don't have a pivot
 - eg.  [1   3   0   0   3]
 	[0  -1   0   0   0] *added
	[0   0   1   0   9]
	[0   0   0   1  -4]
	[0   0   0   0  -1] *added

 - now we can just read those columns and use them (columns 2 and 5 are the same as the previously calculated ones above)

inverse of a matrix:
 - to calculate inverse of matrix, put an identity matrix of same size beside and reduce the original matrix to reduced row echelon form
 - the operations will affect the identity matrix on the right
 - the new resultant matrix on the right will be the desired inverse
 - [A|I] -> [I|A^-1]


algorithms for solving a system of linear equations:
 - gaussian is effective but inefficient for systems with millions of variables as the operations scale cubically
 - there are other ways of solving systems such as: Richardson method, Jacobi method, etc.

2.4 - Vector Spaces ---------------------------------------------------------------------------------------------------

vector spaces:
 -  a structued space in which vectors live

groups:
 - a set of elements and an operation defined on these elements that keeps some structure of the set intact
 - must hold the following qualities:
	- closure of G: xy is element of G 
	- associativity: (xy)z = x(yz)
	- neutral element: xe = x and ex = x
	- inverse element: xy = e and yx = e
		- ** inverse element does not necessarily mean 1/x

abelian group:
 - if xy = yx, then G = (G, dotprod) is an abelian group (commutative)

examples of groups:
 - (Z, +) is a group
 - (N_0, +) is not a group, set of natural numbers including zero
	- the inverse elements are missing so it's not a group
 - (R^nxn, *), it is a group
	- called the general linear group

general linear group:
 - set of regular matrices A which are elements of R^nxn
 - since matrix multiplication is not commutative, the group is not abelian

vector subspaces:
 - a set contained in the original subspace
 - have the property that when we perform vector space operations on elements within the subspace, we will never leave it
 - intersection of arbitrarily many subspaces is a subspace itself

2.5 - Linear Independence ---------------------------------------------------------------------------------------------------

closure property:
 - guarantees that when we add vectors together or multiply with scalar, we end up with another vector in the same vector space

basis:
 - set of vectors with which we can represent every vector in the vector space by adding them together and scaling them

linear combination:
 - say we have finite number of vectors x1, x2, ..., xk which are in the vector space
	- then every vector in the space is of the form v = ax1 + bx2 + ... cx3
	- the a, b, c, .... are constants
	- we can say v is a linear combination of x1, x2, ..., xk

linear independence:
 - vectors can not be represented as a linear combination of other vectors
 - opposite of linear dependence, where we may have redundant vectors

 - example:
	- x1 = [1] 	 x2 = [1]	 x3 = [-1]
	       [2]	      [1]	      [-2]
	       [-3]           [0]             [1]
	       [4]	      [2]             [1]
 - to check if they are linearly dependent:
	- lambda_1 * x_1 + lambda_2 * x_2 + lambda_3 * x_3 = 0
	- write the vectors as a matrix and reduce until we find pivot columns

	- [ 1   1  -1]					[1   1  -1]
	  [ 2   1  -2]		becomes.... -> 		[0   1   0]
	  [-3   0   1]					[0   0   1]
	  [ 4   2   1]					[0   0   0]
	- we can see that every column of the matrix is a pivot column
		- therefore, there is no non-trivial solution
		- therefore, we require lambda_1 = 0, lambda_2 = 0, lambda_3 = 0 to solve
		- therefore x1, x2, x3 are linearly independent
 
 - linear dependent example:
	- we have b1, b2, b3, b4 which are linearly independent vectors
	- x1 = b1 - 2b2 + b3 - b4
	- x2 = -4b1 - 2b2 + 4b4
	- x3 = 2b1 + 3b2 - b3 - 3b4
	- x4 = 17b1 - 10b2 + 11b3 + b4

	- as a matrix:
		- [ 1  -4   2  17]				[1   0   0  -7]
		  [-2  -2   3 -10]  	becomes.... ->		[0   1   0 -15]
		  [ 1   0  -1  11]				[0   0   1 -18]
		  [-1   4  -3   1]				[0   0   0   0]
	- last column is not a pivot column
	- we can represent x4 as linear combination
		- x4 = -7x1 - 15x2 - 18x3
	- therefore x1, x2, x3, x3 are linearly dependent as x4 can be expressed as linear combination of x1, x2, x3

2.6 - Basis and Rank ---------------------------------------------------------------------------------------------------

span, generating set, minimal, basis:
 - let V be vector space and A be set of vectors in V
 - if every vector v in V can expressed as a linear combination of the vectors in A, A is called the generating set
 - the set of all linear combinations of vectors in A is called the span of A
 - if there is smaller set of A that spans V, A is called minimal
 - every linearly independent generating set of V is minimal and is called a basis of V

examples:
 - the standard basis in R^3 is:
	- [1 0 0]
	  [0 1 0]
	  [0 0 1]

 - [1  2  1]
   [2 -1  1]
   [3  0  0]
   [4  2 -4]
	- this set is linearly independent but...
 	- this is not a generating set or basis
	- eg. [1, 0, 0, 0]^T cannot be obtained by a linear combination of elements in that set

basis vectors:
 - every vector space V possesses a basis B
 - there can be many bases of a vector space, there is no unique basis
 - all bases possess the same number of elements, the basis vectors

dimension:
 - the dimension of V is the number of basis vectors of V
 - written as dim(V)
 - can be thought of as the number of independent directions in this vector space
 - V = span([0 1]) is one-dimensional, even though the basis vector possesses two elements

find basis of subspace:
 - write the spanning vectors as columns of a matrix A
 - determine the row-echelon form of A
 - the spanning vectors associated with the pivot columns are a basis of U

determining a basis example:
 - x1 = [1 2 -1 -1 -1]^T
 - x2 = [2 -1 1 2 -2]^T
 - x3 = [3 -4 3 5 -3]^T
 - x4 = [-1 8 -5 -6 1]^T

 - as a matrix:
	- [ 1   2   3  -1]				[1   2   3  -1]
	  [ 2  -1  -4   8]				[0   1   2  -2]
	  [-1   1   3  -5]	 becomes.... ->		[0   0   0   1]
	  [-1   2   5  -6]				[0   0   0   0]
 	  [-1  -2  -3   1]				[0   0   0   0]

 - since x1, x2, x4 have pivot columns, they are linearly independent
 - therefore, {x1, x2, x4} is a basis of U

rank:
 - number of linearly independent columns equals the number of linearly independent rows
	- this is called the rank of A
 - denoted by rk(A)

 - example: 
	      [ 1   2   1]				[1 2 1]
	- A = [-2  -3   1]	 becomes.... ->		[0 1 3]
	      [ 3   5   0]				[0 0 0]
	- we have two linearly independent rows and columnes, therefore rk(A) = 2

 - rank of matrix has many important properties

2.7 - Linear Mappings ---------------------------------------------------------------------------------------------------

linear mapping:
 - say we have vector spaces V and W
 - sigma : V -> W is called a linear mapping if can distribute phi accross x and y like in the example
 - better example: say we are mapping T: R^n -> R^m
	- we have two vectors 'a' and 'b'
 	- it's a linear mapping iff T(a + b) = T(a) + T(b) AND T(ca) = cT(a)

 - mapping is the same thing as a transformation
 - all were doing is mapping one thing to another
	- T([x1, x2]) = [x1+x2, 3x1]
	- T:[x1,x2] -> [x1+x2, 3x1]
	- the above two lines are equivalent

special mappings:
 - say we have a mapping phi: V -> W
 - phi is injective if phi(x) = phi(y) ==> x = y
	- if we have two blobs of area X and Y, each x must map to a unique y (if two x's map to a single y, not injective)
 - phi is surjective if phi(V) = W 
	- if we have a blob of area X and blob of area Y, all this is saying is that everything in the Y area is mapped to from X
 - phi is bijective if it is injective and surjective
 - if phi is surjective, every element in W can be reached from V using phi
 - a bijective mapping can be undone, there is a mapping tht goes W -> V

special cases of linear mappings:
 - isomorphism: phi: V -> W linear and bijective
 - endomorphism: phi: V -> V linear
 - automorphism: phi: V -> V linear and bijective
 - id_v: V -> V, x -> x is the identity mapping in V (aka identity automorphism)

matrix representation of lienar mappings:
 - any n-dimensional vector space is isomorphic to R^n 

ordered basis:
 - B = (b1, b2, ..., bn)

unordered basis:
 - B = {b1, b2, ..., bn}

coordinates:
 - say we have vector space V and basis B = (b1, b2, b3, ..., bn)
 - for any x in the vector space V we have a unique representation of x with respect to B
	- x = alpha_1 * b1 + ... + alpha_b * bn
 - the alpha_n's are the coordinates of x with respect to b
 - the vector alpha = [alpha_1 ... alpha_n]^T is the coordinate vector/representation of x with respect to B

basis:
 - effectively defines a coordinate system
 - eg. [1 2] and [2 1] form a basis for R^2 as every possible combination can be represented with these two vectors
 - a set of B vectors in a vector space V is called a basis if every vector of V may be written in a unique way
	- written as a linear combination of elements of B
	- the coefficients of this linear combination are the 'coordinates'
 - coordinates with respect to different bases will be different even though they make look the same

example of different bases:
 - say we have a vector with coordinates [2 3]^T
 - if we use the standard basis ([1 0]^T [0 1]^T), we can write x = 2e1 + 3e2
 	- e1 and e2 are the two vectors in the standard basis, as they represent the x and y axes
 - if we make a new basis b = (b1, b2) which has b1 = [1 -1]^T and b2 = [1 1]^T
	- we get the coordinates (1/2)[-1, 5]^T to represent the same vector as earlier
	- the new basis is a vector going northeast and another going southeast rather than one north and one east

transformation matrix:
 - T: R^n -> R^n
	- if we have some vector x in R^n, T will map it to R^n and make it T(x)
	- equivalent to T(x) = Ax where A is the matrix
 - A is also called the transformation matrix for T with respect to the standard basis
 - we could say B is the basis for R^n where B = {v1, v2, ..., vn}
 	- every vector in R^n can be represented as a linear combination of B
 - can be used to map coordinates with respect to ordered basis V
 - y = Ax
 - a matrix that can map one vector space to another

example transformation matriecs: 
 - phi(b1) = c1 - c2 + 3c3 - c4
 - phi(b2) = 2c1 + c2 + 7c3 + 2c4
 - phi(b3) = 3c2 + c3 + 4c4
 - transformation matrix A with respect to B and C is:
	      [ 1  2  0]
	- A = [-1  1  3]
	      [ 3  7  1]
	      [-1  2  4]

 - A = [cos(pi/4)  -sin(pi/4)]
       [sin(pi/4)   cos(pi/4)]
 - this matrix will rotate original data by 45 degrees (square to a diamond)

 - A = [2 0]
       [0 1]
 - this matrix will stretch original data along the horizontal axis

basis change:
 - say we have to vector spaces V and W and we map V -> W
 - V has two ordered bases B and B`
 - W has two ordered bases C and C`
 - A will be transformation mapping with respect to bases B and C
 - A` will be transformation mapping with respect to bases B` and C`
 - A` = T^-1 * A * S
 - S is the transformation matrix that maps coordinates with respect to B` onto coordinates with respect to B
 - T is the transformation matrix that maps coordinates with respect to C` onto coordinates with respect to C

equivalent matrices:
 - two matrices A and B are equivalent if there exists regular matrices S and T such that B = T^-1 * A * S

similar matrices:
 - two matrices A and B are similar if there exists a regular matrix S such that B = S^-1 * A * S
 - similar matrices are always equivalent
 - equivalent matrices are not necessarily similar

basis change example:
 - consider a linear mapping 'phi': R^3 -> R^4 with transformation matrix:
	      [ 1  2  0]
	- A = [-1  1  3]
	      [ 3  7  1]
	      [-1  2  4]

 - with respect to standard bases:
	      [1]  [0]  [0]
	- B = [0], [1], [0]
	      [0]  [0]  [1]

	      [1]  [0]  [0]  [0]
	- C = [0], [1], [0], [0]
	      [0]  [0]  [1]  [0]
	      [0]  [0]  [0]  [1]

 - we seek the transformation matrix A` with respect to the new bases:
	       [1]  [0]  [1]
	- B` = [1], [1], [0]
	       [0]  [1]  [1]

	       [1]  [1]  [0]  [1]
	- C` = [1], [0], [1], [0]
	       [0]  [1]  [1]  [0]
	       [0]  [0]  [0]  [1]

 - then from B` and C` we get S and T:
	      [1 0 1]
	- S = [1 1 0]
	      [0 1 1]

	      [1 1 0 1]
	- T = [1 0 1 0]
	      [0 1 1 0]
	      [0 0 0 1]

 - we then solve A` = T^-1 * A * S
	       [-4  -4  -2]
	- A` = [ 6   0   0]
	       [ 4   8   4]
	       [ 1   6   3]
 - this is the new transformation matrix

image and kernel:
 - if phi is the linear mapping from V -> W, the kernel is:
	- ker(phi) = every vector v in the vector space V such that phi(v) = 0_w
 - kernel is the set of vectors v in V such that the transformation of the vectors v will be equal to 0 when mapped to w
 - eg. vectors in area V are 0 in the vector space W
 - for example, say we have matrix:
	- [1 0 0]
	  [0 2 1]
	- the kernel of the above matrix is: [ 0]
					     [-s]
					     [2s]
	- for regardless of what s value we have, multiplying the kernel by the matrix yields matrix of zeroes

 - the image is:
	- Im(phi) = every vector w in the vector space W and a single vector v in vector space V where phi(v) = w
 - image of vector space V onto W is the area in W that is mapped from V (that's why the image has vars, because it's a span/plane)
 - for example, say we have matrix below and multiply by sample variables s and t:
	- [1 0]	  [s]	[s]
	  [0 2] * [t] = [2t] = Im(A)
	  [0 1]		[t]
	- this is the image of the matrix A above

 - V is called the domain phi, W is called the codomain of phi
 - rk(A) = dim(Im(phi))
 - kernel (also known as null space) is the general solution to Ax = 0

another example of kernel and image of a transformation matrix:
 - say we map 'phi': R^4 -> R^2,
	  [x1]	   	       [x1]
	- [x2] -> [1  2 -1  0] [x2] = [x1 + 2x2 - x3]  (2x1 transformation matrix)
	  [x3]    [1  0  0  1] [x3]   [x1 + x4]
	  [x4]	   	       [x4]

 - to determine the Im(phi), take the span of the columns:
	- Im(phi) = span([1], [2], [-1], [0])
		  	 [1]  [0]  [0]   [1]

rank-nullity theorem:
 - say we have a mapping phi: V -> W
	- dim(ker(phi)) + dim(Im(phi)) = dim(V)
 - also known as the fundamental theorem of linear mappings

2.8 - Affine Spaces ---------------------------------------------------------------------------------------------------

affine spaces:
 - spaces that are offset from the origin
 - spaces that are no longer vector subspaces

affine transformation:
 - combination of linear transformation with translations
 - origin does not necessarily map to the origin
 - lines still map to lines
 - parallel lines remain parallel
 - think of a picture that was skewed, picture is still intact but warped

hyperplane:
 - a subspace whose dimension is one less than that of its ambient space
 - eg. 1d-line in 2d-space, 2d-plane in 3d-space

names:
 - 1-dimensional affine spaces are LINES
 - 2-dimensional affine spaces are PLANES
 - (n-1)-dimensional affine spaes are HYPERPLANES