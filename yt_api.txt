import json
import urllib.request
import nltk
from pprint import pprint
from sklearn.feature_extraction.text import CountVectorizer

api_key = r"AIzaSyDmVJu0dTtlKe36VTd2hmywO5pzPpSsGn4"


def get_videos_from_channel(channel_id):
    video_ids = []
    channel_videos = urllib.request.urlopen(f"https://www.googleapis.com/youtube/v3/search?part=snippet&"
                                            f"channelId={channel_id}&type=video&order=date&maxResults=10&key={api_key}")
    channel_videos_json = json.loads(channel_videos.read())['items']
    for x in channel_videos_json:
        video_ids.append(x['id']['videoId'])
    return video_ids


# print(get_videos_from_channel("UCfzlCWGWYyIQ0aLC5w48gBQ"))


class Video:
    def __init__(self, video_id):
        metadata_url = f"https://www.googleapis.com/youtube/v3/videos?part=snippet&id={video_id}&key={api_key}"
        statistics_url = f"https://www.googleapis.com/youtube/v3/videos?part=statistics&id={video_id}&key={api_key}"
        metadata_url = urllib.request.urlopen(metadata_url)
        statistics_url = urllib.request.urlopen(statistics_url)

        self.metadata_info = json.loads(metadata_url.read())
        self.statistics_info = json.loads(statistics_url.read())

        channel_id = self.metadata_info['items'][0]['snippet']['channelId']
        channel_url = urllib.request.urlopen(f"https://www.googleapis.com/youtube/v3/channels?part=statistics&id="
                                             f"{channel_id}&key={api_key}")

        self.channel_info = json.loads(channel_url.read())

    def get_video_title(self):
        return self.metadata_info["items"][0]["snippet"]["title"]

    def get_thumbnail(self):
        return self.metadata_info['items'][0]['snippet']['thumbnails']['default']['url']

    def get_view_count(self):
        return self.statistics_info['items'][0]['statistics']['viewCount']

    def get_subscriber_count(self):
        return self.channel_info['items'][0]['statistics']['subscriberCount']


# key: video_id
# value: [title, subs, thumb, views]
videos = ['8Z8nkjmzC14', 'xStvKju-4-o', 'Dgo7F-lpyYE', '6gk7giKER6s', '1gQR24B3ISE', 'p9bkz3hxrSM', '9aYuQmMJvjA',
          '9j-_dOze4IM', 'ixathu7U-LQ', 'i2yPxY2rOzs']
training_data = {}

for video in videos:
    vid_data = Video(video)
    training_data[video] = (vid_data.get_video_title().lower(),
                            vid_data.get_subscriber_count(),
                            vid_data.get_thumbnail(),
                            vid_data.get_view_count())

positive = []
negative = []

for i in training_data:
    ratio = int(training_data[i][3])/int(training_data[i][1])
    tokenize = nltk.word_tokenize(training_data[i][0])
    for j in tokenize:
        if ratio > 0.2:
            positive.append(j)
        else:
            negative.append(j)

tokens = nltk.word_tokenize(training_data['8Z8nkjmzC14'][0])
tagged = nltk.pos_tag(tokens)
named_ent = nltk.ne_chunk(tagged, binary=True)

freq_dist = nltk.FreqDist(positive)
print(freq_dist.most_common(10))
print(negative)
